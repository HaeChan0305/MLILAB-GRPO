import os
import re
import glob
import shutil
import subprocess
from huggingface_hub import HfApi, upload_folder, snapshot_download


# Hugging Faceì— ë¡œê·¸ì¸ëœ ì‚¬ìš©ì í† í° í™•ì¸
api = HfApi()


def upload_model(experiment_name, repo_id, token, step, folder_path=None, inference_only=True, convert_to_safetensors=False):
    """
    ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ Hugging Faceì— ì—…ë¡œë“œ
    
    Args:
        experiment_name: ì‹¤í—˜ ì´ë¦„
        repo_id: HuggingFace ì €ì¥ì†Œ ID
        token: HuggingFace í† í°
        step: ì—…ë¡œë“œí•  step
        folder_path: ì—…ë¡œë“œí•  ëª¨ë¸ í´ë” ê²½ë¡œ (ê¸°ë³¸ê°’: ./models/{experiment_name}/global_step_{step})
        inference_only: Trueë©´ inferenceì— í•„ìš”í•œ íŒŒì¼ë§Œ ì—…ë¡œë“œ (ê¸°ë³¸ê°’ True)
                       - huggingface/ í´ë”
                       - model_world_size_*.pt íŒŒì¼
                       Falseë©´ ëª¨ë“  íŒŒì¼ ì—…ë¡œë“œ (optim, extra_state ë“± í¬í•¨)
        convert_to_safetensors: Trueë©´ ì—…ë¡œë“œ ì „ safetensors í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ê¸°ë³¸ê°’ False)
                               FSDP sharded checkpoint (.pt)ë¥¼ HuggingFace safetensorsë¡œ ë³€í™˜
    """
    if folder_path is None:
        folder_path = f"./models/{experiment_name}/global_step_{step}"
    
    # Convert to safetensors format before upload if requested
    if convert_to_safetensors:
        actor_path = os.path.join(folder_path, "actor")
        print(f"ğŸ”„ Converting to safetensors format before upload...")
        convert_to_safetensors_format(actor_path)
    
    # Inferenceì— ë¶ˆí•„ìš”í•œ íŒŒì¼ íŒ¨í„´ (training resumeì—ë§Œ í•„ìš”)
    ignore_patterns = None
    if inference_only:
        ignore_patterns = [
            "actor/optim_world_size_*",       # ì˜µí‹°ë§ˆì´ì € ìƒíƒœ
            "actor/extra_state_world_size_*", # í•™ìŠµ ìŠ¤í…, ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ
            "fsdp_config.json",         # FSDP ì„¤ì •
            "data.pt",                  # ê¸°íƒ€ training ë°ì´í„°
        ]
        # safetensorsë¡œ ë³€í™˜í•œ ê²½ìš° .pt íŒŒì¼ë„ ì œì™¸
        if convert_to_safetensors:
            ignore_patterns.append("actor/model_world_size_*")  # ì›ë³¸ .pt íŒŒì¼ ì œì™¸
        print(f"ğŸ“¦ Inference-only mode: excluding {ignore_patterns}")
    
    # ë¦¬í¬ì§€í† ë¦¬ ìƒì„± (ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¬´ì‹œë¨)
    api.create_repo(repo_id=repo_id, exist_ok=True, token=token)

    # chmod 777 ì ìš©
    print(f"Applying chmod 777 to {folder_path}")
    subprocess.run(["sudo", "chmod", "-R", "777", folder_path], check=True)

    print(f"Uploading step {step}")
    upload_folder(
        folder_path=folder_path,      # ì—…ë¡œë“œí•  ë¡œì»¬ ëª¨ë¸ í´ë” ê²½ë¡œ
        repo_id=repo_id,             # ì—…ë¡œë“œí•  Hugging Face repo ì´ë¦„
        repo_type="model",           # model / dataset / space ì¤‘ ì„ íƒ
        commit_message=f"Upload global_step_{step}",
        token=token,
        ignore_patterns=ignore_patterns,
    )
    print(f"Uploaded step {step} (inference-only: {inference_only}, safetensors: {convert_to_safetensors})")




def get_step_from_commit_message(message: str) -> int | None:
    """
    ì»¤ë°‹ ë©”ì‹œì§€ì—ì„œ step ë²ˆí˜¸ ì¶”ì¶œ
    ì§€ì›í•˜ëŠ” í˜•ì‹:
    - 'Upload step 105' -> 105
    - 'Upload global_step_105' -> 105
    - 'Upload global_step_105 (inference-only)' -> 105
    """
    patterns = [
        r'Upload global_step_(\d+)',  # Upload global_step_105
        r'Upload step (\d+)',          # Upload step 105
        r'global_step_(\d+)',          # global_step_105
        r'step[_\-\s]?(\d+)',          # step_105, step-105, step 105
    ]
    
    for pattern in patterns:
        match = re.search(pattern, message, re.IGNORECASE)
        if match:
            return int(match.group(1))
    return None


def list_checkpoint_commits(repo_id: str, token: str = None, max_commits: int = 200):
    """ì €ì¥ì†Œì˜ ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ ì»¤ë°‹ ëª©ë¡ ì¡°íšŒ (Git clone ë°©ì‹)"""
    import tempfile
    import shutil
    
    all_commits = []
    
    # Gitì„ ì‚¬ìš©í•˜ì—¬ ì»¤ë°‹ íˆìŠ¤í† ë¦¬ ì¡°íšŒ
    with tempfile.TemporaryDirectory() as temp_dir:
        repo_url = f"https://huggingface.co/{repo_id}"
        if token:
            # í† í°ì´ ìˆìœ¼ë©´ ì¸ì¦ URL ì‚¬ìš©
            repo_url = f"https://USER:{token}@huggingface.co/{repo_id}"
        
        clone_dir = os.path.join(temp_dir, "repo")
        
        try:
            # Shallow clone with only git history (no files)
            print(f"ğŸ“¥ Fetching commit history from {repo_id}...")
            result = subprocess.run(
                ["git", "clone", "--bare", "--filter=blob:none", repo_url, clone_dir],
                capture_output=True,
                text=True,
                timeout=60
            )
            
            if result.returncode != 0:
                print(f"âš ï¸  Git clone failed: {result.stderr}")
                # Fallback to huggingface_hub
                return _list_commits_fallback(repo_id, token, max_commits)
            
            # Git logë¡œ ì»¤ë°‹ íˆìŠ¤í† ë¦¬ ì¡°íšŒ
            result = subprocess.run(
                ["git", "-C", clone_dir, "log", "--format=%H|%s|%aI", f"-{max_commits}"],
                capture_output=True,
                text=True
            )
            
            if result.returncode != 0:
                print(f"âš ï¸  Git log failed: {result.stderr}")
                return _list_commits_fallback(repo_id, token, max_commits)
            
            # ì»¤ë°‹ íŒŒì‹±
            class CommitInfo:
                def __init__(self, commit_id, title, created_at):
                    self.commit_id = commit_id
                    self.title = title
                    self.created_at = created_at
            
            for line in result.stdout.strip().split("\n"):
                if not line:
                    continue
                parts = line.split("|", 2)
                if len(parts) >= 2:
                    commit_id = parts[0]
                    title = parts[1]
                    created_at = parts[2] if len(parts) > 2 else ""
                    all_commits.append(CommitInfo(commit_id, title, created_at))
            
        except subprocess.TimeoutExpired:
            print("âš ï¸  Git clone timeout, using fallback...")
            return _list_commits_fallback(repo_id, token, max_commits)
        except Exception as e:
            print(f"âš ï¸  Error: {e}")
            return _list_commits_fallback(repo_id, token, max_commits)
    
    print(f"ğŸ“‹ Total commits fetched: {len(all_commits)}")
    
    checkpoint_commits = []
    for commit in all_commits:
        step = get_step_from_commit_message(commit.title)
        checkpoint_commits.append({
            'commit_id': commit.commit_id,
            'title': commit.title,
            'step': step,
            'created_at': commit.created_at
        })
    
    return checkpoint_commits


def _list_commits_fallback(repo_id: str, token: str = None, max_commits: int = 200):
    """Fallback: huggingface_hub ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©"""
    from huggingface_hub import list_repo_commits
    
    all_commits = []
    
    try:
        for commit in list_repo_commits(repo_id, token=token, repo_type="model"):
            all_commits.append(commit)
            if len(all_commits) >= max_commits:
                break
    except Exception as e:
        print(f"âš ï¸  Fallback also failed: {e}")
    
    print(f"ğŸ“‹ Total commits fetched (fallback): {len(all_commits)}")
    
    checkpoint_commits = []
    for commit in all_commits:
        step = get_step_from_commit_message(commit.title)
        checkpoint_commits.append({
            'commit_id': commit.commit_id,
            'title': commit.title,
            'step': step,
            'created_at': commit.created_at
        })
    
    return checkpoint_commits

def download_model(experiment_name, repo_id, step, token, convert_to_safetensors=True):
    # list_checkpoint_commitsë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ì»¤ë°‹ ì¡°íšŒ (í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬ë¨)
    commits = list_checkpoint_commits(repo_id, token)
    commit_message = None
    commit_id = None
    
    for commit in commits:
        if commit['step'] == step:
            commit_message = commit['title']
            commit_id = commit['commit_id']
            break
    
    if commit_message is None:
        available_steps = sorted([c['step'] for c in commits if c['step'] is not None])
        raise ValueError(f"Step {step} not found in repo {repo_id}. Available steps: {available_steps}")
    
    print(f"Commit message: {commit_message}")
    print(f"Commit id: {commit_id}")
    
    
    folder_path = f"./models/{experiment_name}/global_step_{step}"
    os.makedirs(folder_path, exist_ok=True)
    
    # repo ì´ë¦„ê³¼ commitì„ ì£¼ë©´ huggingface hubì—ì„œ í•´ë‹¹ commitì˜ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ ë°›ëŠ”ë‹¤.
    snapshot_download(
        repo_id=repo_id,
        revision=commit_id,
        token=token,
        local_dir=folder_path
    )
    
    print(f"âœ… Model successfully downloaded to {folder_path}")
    
    # Convert to safetensors format
    if convert_to_safetensors:
        actor_path = os.path.join(folder_path, "actor")
        convert_to_safetensors_format(actor_path)
    
    return folder_path


def convert_to_safetensors_format(actor_path):
    """
    Convert FSDP sharded checkpoint (.pt files) to HuggingFace safetensors format.
    Uses veRL's model_merger for proper handling of DTensor and sharded checkpoints.
    
    Expected structure:
    - actor_path/model_world_size_X_rank_Y.pt (FSDP sharded model weights)
    - actor_path/huggingface/ (tokenizer and config files)
    
    Output:
    - actor_path/huggingface/*.safetensors (converted model weights)
    """
    from verl.model_merger.base_model_merger import ModelMergerConfig
    from verl.model_merger.fsdp_model_merger import FSDPModelMerger
    
    huggingface_path = os.path.join(actor_path, "huggingface")
    
    # Check if already converted
    safetensor_files = glob.glob(os.path.join(huggingface_path, "*.safetensors"))
    if safetensor_files:
        print(f"Model at {huggingface_path} is already in safetensors format.")
        return huggingface_path
    
    # Check for FSDP sharded checkpoint files
    pt_files = sorted(glob.glob(os.path.join(actor_path, "model_world_size_*_rank_*.pt")))
    
    if not pt_files:
        print(f"No FSDP checkpoint files found at {actor_path}. Skipping conversion.")
        return None
    
    print(f"Found {len(pt_files)} FSDP sharded checkpoint files.")
    print(f"Converting FSDP checkpoint to safetensors format using veRL model_merger...")
    
    # Temporarily allow loading old pickle format (PyTorch 2.6+ compatibility)
    old_env = os.environ.get("TORCH_FORCE_WEIGHTS_ONLY_LOAD")
    os.environ["TORCH_FORCE_WEIGHTS_ONLY_LOAD"] = "0"
    
    try:
        # Use veRL's FSDPModelMerger
        config = ModelMergerConfig(
            operation="merge",
            backend="fsdp",
            local_dir=actor_path,
            target_dir=huggingface_path,
            hf_model_config_path=huggingface_path,  # tokenizer/config files location
            trust_remote_code=True,
        )
        
        merger = FSDPModelMerger(config)
        merger.merge_and_save()
        merger.cleanup()
        
        print(f"âœ… Model successfully converted to safetensors format: {huggingface_path}")
        return huggingface_path
    
    finally:
        # Restore original environment
        if old_env is None:
            os.environ.pop("TORCH_FORCE_WEIGHTS_ONLY_LOAD", None)
        else:
            os.environ["TORCH_FORCE_WEIGHTS_ONLY_LOAD"] = old_env


def move_id2score_json_files(experiment_name):
    base_dir = f"/wbl/post-training/haechan_workspace/MLILAB-GRPO/models/{experiment_name}"

    # id2score_step_*.json íŒŒì¼ë“¤ ì°¾ê¸°
    json_files = glob.glob(os.path.join(base_dir, "id2score_step_*.json"))

    moved_count = 0
    skipped_count = 0

    for json_file in sorted(json_files):
        filename = os.path.basename(json_file)
        
        # step ë²ˆí˜¸ ì¶”ì¶œ: id2score_step_100.json -> 100
        step = filename.replace("id2score_step_", "").replace(".json", "")
        
        # ëŒ€ìƒ í´ë”
        target_dir = os.path.join(base_dir, f"global_step_{step}")
        
        if os.path.isdir(target_dir):
            # í´ë”ê°€ ì¡´ì¬í•˜ë©´ ì´ë™
            target_path = os.path.join(target_dir, filename)
            shutil.move(json_file, target_path)
            print(f"âœ“ Moved: {filename} -> global_step_{step}/")
            moved_count += 1
        else:
            # í´ë”ê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€
            print(f"â­ Skipped: {filename} (global_step_{step}/ not found)")
            skipped_count += 1

    print(f"\nğŸ“Š Summary: {moved_count} moved, {skipped_count} skipped")


if __name__ == "__main__":
    token = None
    exp_name = "qwen3-dr-grpohistbeta-paper-batch128-cliph1_0-clipl1_0-clipc10-nokl-lr1e-6-df0_5"
    repo_id = f"HaeChan0305/{exp_name}"
    folder_path = f"/home/jovyan/haechan_workspace/verl/models/{exp_name}/global_step_240"
    upload_model(exp_name, repo_id, token, step=240, folder_path=folder_path, inference_only=True, convert_to_safetensors=False)